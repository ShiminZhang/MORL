{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFWxv9AIqSqj",
        "outputId": "607599e0-fed2-4333-e1ff-307ea1c2f819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "4\n",
            "Box(-inf, inf, (6,), float32)\n",
            "Starting Training (Variant A: Reward Scalarization)...\n",
            "Update 10/625, Loss: 0.8019, Mean Reward: 0.24\n",
            "Update 20/625, Loss: 1.8382, Mean Reward: 0.26\n",
            "Update 30/625, Loss: 1.1735, Mean Reward: 0.25\n",
            "Update 40/625, Loss: 1.7226, Mean Reward: 0.27\n",
            "Update 50/625, Loss: 1.4436, Mean Reward: 0.25\n",
            "Update 60/625, Loss: 2.1940, Mean Reward: 0.28\n",
            "Update 70/625, Loss: 5.4131, Mean Reward: 0.42\n",
            "Update 80/625, Loss: 2.5732, Mean Reward: 0.35\n",
            "Update 90/625, Loss: 0.8587, Mean Reward: 0.26\n",
            "Update 100/625, Loss: 1.8339, Mean Reward: 0.30\n",
            "Update 110/625, Loss: 0.8233, Mean Reward: 0.27\n",
            "Update 120/625, Loss: 4.1180, Mean Reward: 0.32\n",
            "Update 130/625, Loss: 0.1123, Mean Reward: 0.15\n",
            "Update 140/625, Loss: 0.4919, Mean Reward: 0.26\n",
            "Update 150/625, Loss: 4.4566, Mean Reward: 0.25\n",
            "Update 160/625, Loss: 4.5162, Mean Reward: 0.26\n",
            "Update 170/625, Loss: 3.2008, Mean Reward: 0.25\n",
            "Update 180/625, Loss: 0.6855, Mean Reward: 0.29\n",
            "Update 190/625, Loss: 0.2180, Mean Reward: 0.24\n",
            "Update 200/625, Loss: 0.3333, Mean Reward: 0.26\n",
            "Update 210/625, Loss: 4.6938, Mean Reward: 0.29\n",
            "Update 220/625, Loss: 0.8057, Mean Reward: 0.09\n",
            "Update 230/625, Loss: 0.0628, Mean Reward: 0.22\n",
            "Update 240/625, Loss: 5.6527, Mean Reward: 0.22\n",
            "Update 250/625, Loss: 0.0289, Mean Reward: 0.19\n",
            "Update 260/625, Loss: 0.1970, Mean Reward: 0.25\n",
            "Update 270/625, Loss: 3.5014, Mean Reward: 0.25\n",
            "Update 280/625, Loss: 0.2771, Mean Reward: 0.18\n",
            "Update 290/625, Loss: 0.8149, Mean Reward: 0.30\n",
            "Update 300/625, Loss: 7.5792, Mean Reward: 0.14\n",
            "Update 310/625, Loss: 0.2919, Mean Reward: 0.26\n",
            "Update 320/625, Loss: 5.4173, Mean Reward: 0.22\n",
            "Update 330/625, Loss: 1.0638, Mean Reward: 0.31\n",
            "Update 340/625, Loss: 4.8144, Mean Reward: 0.28\n",
            "Update 350/625, Loss: 0.0607, Mean Reward: 0.18\n",
            "Update 360/625, Loss: 0.2115, Mean Reward: 0.25\n",
            "Update 370/625, Loss: 0.2260, Mean Reward: 0.26\n",
            "Update 380/625, Loss: 5.7665, Mean Reward: 0.24\n",
            "Update 390/625, Loss: 0.2520, Mean Reward: 0.13\n",
            "Update 400/625, Loss: 0.2281, Mean Reward: 0.25\n",
            "Update 410/625, Loss: 0.1729, Mean Reward: 0.25\n",
            "Update 420/625, Loss: 0.3239, Mean Reward: 0.27\n",
            "Update 430/625, Loss: 5.5516, Mean Reward: 0.21\n",
            "Update 440/625, Loss: 0.0354, Mean Reward: 0.19\n",
            "Update 450/625, Loss: 5.4241, Mean Reward: 0.24\n",
            "Update 460/625, Loss: 5.4569, Mean Reward: 0.27\n",
            "Update 470/625, Loss: 0.2339, Mean Reward: 0.25\n",
            "Update 480/625, Loss: 0.2619, Mean Reward: 0.12\n",
            "Update 490/625, Loss: 0.2490, Mean Reward: 0.23\n",
            "Update 500/625, Loss: 0.1928, Mean Reward: 0.25\n",
            "Update 510/625, Loss: 0.0815, Mean Reward: 0.23\n",
            "Update 520/625, Loss: 0.2985, Mean Reward: 0.26\n",
            "Update 530/625, Loss: 0.1289, Mean Reward: 0.23\n",
            "Update 540/625, Loss: 0.0013, Mean Reward: 0.20\n",
            "Update 550/625, Loss: 5.9031, Mean Reward: 0.25\n",
            "Update 560/625, Loss: 0.1495, Mean Reward: 0.25\n",
            "Update 570/625, Loss: 3.3843, Mean Reward: 0.21\n",
            "Update 580/625, Loss: 0.0291, Mean Reward: 0.15\n",
            "Update 590/625, Loss: 0.2467, Mean Reward: 0.26\n",
            "Update 600/625, Loss: 0.1915, Mean Reward: 0.25\n",
            "Update 610/625, Loss: 0.0268, Mean Reward: 0.16\n",
            "Update 620/625, Loss: 0.1068, Mean Reward: 0.23\n",
            "Training Finished!\n",
            "\n",
            "=== Running Verification (Variant A) ===\n",
            "4\n",
            "Box(-inf, inf, (6,), float32)\n",
            "Weight (w1, w2)      | Avg Position    | Steps     \n",
            "--------------------------------------------------\n",
            "[1.0, 0.0]           |  1.1749          | 500       \n",
            "[0.7, 0.3]           |  0.5704          | 500       \n",
            "[0.5, 0.5]           |  0.1471          | 500       \n",
            "[0.3, 0.7]           | -0.0924          | 500       \n",
            "[0.0, 1.0]           | -0.9143          | 500       \n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "# ==========================================\n",
        "# Variant A: Reward-Space Combination\n",
        "# ==========================================\n",
        "class ScalarRewardWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        original_shape = env.observation_space.shape[0]\n",
        "        print(original_shape)\n",
        "        # add 2 dimensions for w (w1, w2)\n",
        "        # 4 + 2 = 6\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=-np.inf, high=np.inf,\n",
        "            shape=(original_shape + 2,),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "        print(self.observation_space)\n",
        "        # initialize w\n",
        "        self.current_w = np.array([0.5, 0.5], dtype=np.float32)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        if options and 'w' in options:\n",
        "            self.current_w = np.array(options['w'], dtype=np.float32)\n",
        "        else:\n",
        "            w = np.random.rand(2)\n",
        "            self.current_w = w / w.sum()\n",
        "\n",
        "        obs, info = self.env.reset(seed=seed)\n",
        "        return np.concatenate([obs, self.current_w]), info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, _, terminated, truncated, info = self.env.step(action)\n",
        "        x, x_dot, theta, theta_dot = obs\n",
        "\n",
        "        # x range [-2.4, 2.4]\n",
        "        norm_left = (2.4 - x) / 4.8\n",
        "        norm_right = (x + 2.4) / 4.8\n",
        "\n",
        "        r1 = norm_left ** 2\n",
        "        r2 = norm_right ** 2\n",
        "\n",
        "        if terminated:\n",
        "            r1 = 0.0\n",
        "            r2 = 0.0\n",
        "\n",
        "        # scalarize the reward vector\n",
        "        # Reward = w1 * r1 + w2 * r2\n",
        "        scalar_reward = self.current_w[0] * r1 + self.current_w[1] * r2\n",
        "\n",
        "        new_obs = np.concatenate([obs, self.current_w])\n",
        "\n",
        "        # only know scalar_reward\n",
        "        return new_obs, scalar_reward, terminated, truncated, info\n",
        "\n",
        "# ==========================================\n",
        "# 2. PPO Agent\n",
        "# ==========================================\n",
        "# orthogonal\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        # Critic：estimate V(s)\n",
        "        self.critic = nn.Sequential(\n",
        "            layer_init(nn.Linear(6, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 1), std=1.0),\n",
        "        )\n",
        "        # Actor：action prob distribution\n",
        "        self.actor = nn.Sequential(\n",
        "            layer_init(nn.Linear(6, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, env.action_space.n), std=0.01),\n",
        "        )\n",
        "\n",
        "    def get_value(self, x):\n",
        "        # get Value\n",
        "        return self.critic(x)\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        logits = self.actor(x)\n",
        "        probs = Categorical(logits=logits)\n",
        "        if action is None:\n",
        "            action = probs.sample()\n",
        "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
        "\n",
        "# ==========================================\n",
        "# 3. main loop\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    # hyperparameters\n",
        "    learning_rate = 3e-4\n",
        "    num_steps = 128\n",
        "    total_timesteps = 80000\n",
        "    gamma = 0.99\n",
        "    gae_lambda = 0.95\n",
        "    update_epochs = 4\n",
        "    clip_coef = 0.2\n",
        "    ent_coef = 0.001\n",
        "    vf_coef = 0.5\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize env and agent\n",
        "    env = ScalarRewardWrapper(gym.make(\"CartPole-v1\"))\n",
        "    agent = Agent(env).to(device)\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
        "\n",
        "    # initialize Buffer\n",
        "    obs = torch.zeros((num_steps, 6)).to(device)\n",
        "    actions = torch.zeros((num_steps,)).to(device)\n",
        "    logprobs = torch.zeros((num_steps,)).to(device)\n",
        "    rewards = torch.zeros((num_steps,)).to(device)\n",
        "    dones = torch.zeros((num_steps,)).to(device)\n",
        "    values = torch.zeros((num_steps,)).to(device)\n",
        "\n",
        "    global_step = 0\n",
        "    next_obs, _ = env.reset()\n",
        "    next_obs = torch.Tensor(next_obs).to(device)\n",
        "    next_done = torch.zeros(1).to(device)\n",
        "\n",
        "    num_updates = total_timesteps // num_steps\n",
        "\n",
        "    print(\"Starting Training (Variant A: Reward Scalarization)...\")\n",
        "\n",
        "    for update in range(1, num_updates + 1):\n",
        "\n",
        "        # Rollout\n",
        "        for step in range(num_steps):\n",
        "            global_step += 1\n",
        "            # save current observation\n",
        "            obs[step] = next_obs\n",
        "            dones[step] = next_done\n",
        "\n",
        "            with torch.no_grad():\n",
        "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
        "                values[step] = value.flatten()\n",
        "\n",
        "            actions[step] = action\n",
        "            logprobs[step] = logprob\n",
        "\n",
        "            real_next_obs, reward, terminated, truncated, info = env.step(action.item())\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # save reward\n",
        "            rewards[step] = torch.tensor(reward).to(device)\n",
        "            # update next_obs\n",
        "            next_obs = torch.Tensor(real_next_obs).to(device)\n",
        "            next_done = torch.tensor(float(done)).to(device)\n",
        "\n",
        "            if done:\n",
        "                next_obs_np, _ = env.reset()\n",
        "                next_obs = torch.Tensor(next_obs_np).to(device)\n",
        "\n",
        "        # GAE\n",
        "        with torch.no_grad():\n",
        "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
        "            # initialize\n",
        "            advantages = torch.zeros_like(rewards).to(device)\n",
        "            lastgaelam = 0\n",
        "\n",
        "            for t in reversed(range(num_steps)):\n",
        "                if t == num_steps - 1:\n",
        "                    nextnonterminal = 1.0 - next_done\n",
        "                    nextvalues = next_value\n",
        "                else:\n",
        "                    nextnonterminal = 1.0 - dones[t + 1]\n",
        "                    nextvalues = values[t + 1]\n",
        "\n",
        "                # TD Error = r + gamma * V(s') - V(s)\n",
        "                delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
        "                # GAE = delta + gamma * lambda * GAE(t+1)\n",
        "                advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
        "\n",
        "            # Returns = Advantage + Value\n",
        "            returns = advantages + values\n",
        "\n",
        "        # PPO Update\n",
        "        b_obs = obs\n",
        "        b_logprobs = logprobs\n",
        "        b_actions = actions\n",
        "        b_advantages = advantages\n",
        "        b_returns = returns\n",
        "        b_values = values\n",
        "\n",
        "        for epoch in range(update_epochs):\n",
        "            _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs, b_actions)\n",
        "            #  ratio = new / old\n",
        "            logratio = newlogprob - b_logprobs\n",
        "            ratio = logratio.exp()\n",
        "\n",
        "            # Advantage Normalization\n",
        "            mb_advantages = (b_advantages - b_advantages.mean()) / (b_advantages.std() + 1e-8)\n",
        "\n",
        "            # Policy Loss\n",
        "            pg_loss1 = -mb_advantages * ratio\n",
        "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
        "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "            # Value Loss (MSE Loss)\n",
        "            v_loss = 0.5 * ((newvalue.view(-1) - b_returns) ** 2).mean()\n",
        "\n",
        "            # total Loss\n",
        "            loss = pg_loss - ent_coef * entropy.mean() + vf_coef * v_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if update % 10 == 0:\n",
        "            print(f\"Update {update}/{num_updates}, Loss: {loss.item():.4f}, Mean Reward: {rewards.mean().item():.2f}\")\n",
        "\n",
        "    print(\"Training Finished!\")\n",
        "    # save model weight\n",
        "    torch.save(agent.state_dict(), \"variant_a_agent.pth\")\n",
        "\n",
        "\n",
        "    # ==========================================\n",
        "    # 4. Verification\n",
        "    # ==========================================\n",
        "    print(\"\\n=== Running Verification (Variant A) ===\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        demo_env = ScalarRewardWrapper(gym.make(\"CartPole-v1\"))\n",
        "    except NameError:\n",
        "        demo_env = ScalarRewardWrapper(gym.make(\"CartPole-v1\"))\n",
        "\n",
        "    agent.eval()\n",
        "\n",
        "    test_weights = [\n",
        "        [1.0, 0.0],  # extreme left\n",
        "        [0.7, 0.3],  # left\n",
        "        [0.5, 0.5],  # middle\n",
        "        [0.3, 0.7],  # right\n",
        "        [0.0, 1.0]   # extreme right\n",
        "    ]\n",
        "\n",
        "    print(f\"{'Weight (w1, w2)':<20} | {'Avg Position':<15} | {'Steps':<10}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for w in test_weights:\n",
        "        obs, _ = demo_env.reset(options={'w': w})\n",
        "        obs = torch.Tensor(obs).to(device)\n",
        "\n",
        "        positions = []\n",
        "        steps = 0\n",
        "\n",
        "        while True:\n",
        "            with torch.no_grad():\n",
        "                action, _, _, _ = agent.get_action_and_value(obs)\n",
        "\n",
        "            step_result = demo_env.step(action.item())\n",
        "            real_next_obs = step_result[0]\n",
        "            term = step_result[2]\n",
        "            trunc = step_result[3]\n",
        "\n",
        "            positions.append(real_next_obs[0])\n",
        "            steps += 1\n",
        "\n",
        "            obs = torch.Tensor(real_next_obs).to(device)\n",
        "            if term or trunc: break\n",
        "\n",
        "        avg_pos = np.mean(positions)\n",
        "        print(f\"{str(w):<20} | {avg_pos: .4f}          | {steps:<10}\")\n",
        "\n",
        "    demo_env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "# ==========================================\n",
        "# Variant B: Value/Q-Space Combination\n",
        "# ==========================================\n",
        "class SteerableCartPoleWrapper(gym.Wrapper):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        original_shape = env.observation_space.shape[0]\n",
        "        # state space: 4+2=6 (State + w)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=-np.inf, high=np.inf,\n",
        "            shape=(original_shape + 2,),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "        self.current_w = np.array([0.5, 0.5], dtype=np.float32)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        if options and 'w' in options:\n",
        "            self.current_w = np.array(options['w'], dtype=np.float32)\n",
        "        else:\n",
        "            w = np.random.rand(2)\n",
        "            self.current_w = w / w.sum()\n",
        "\n",
        "        obs, info = self.env.reset(seed=seed)\n",
        "        # combine State + w\n",
        "        return np.concatenate([obs, self.current_w]), info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, _, terminated, truncated, info = self.env.step(action)\n",
        "        x, x_dot, theta, theta_dot = obs\n",
        "\n",
        "        norm_left = (2.4 - x) / 4.8\n",
        "        norm_right = (x + 2.4) / 4.8\n",
        "\n",
        "        r1 = norm_left ** 2\n",
        "        r2 = norm_right ** 2\n",
        "\n",
        "        if terminated:\n",
        "            r1 = 0.0\n",
        "            r2 = 0.0\n",
        "\n",
        "        # reward as a vector\n",
        "        vec_reward = np.array([r1, r2], dtype=np.float32)\n",
        "\n",
        "        # save into info\n",
        "        info['vec_reward'] = vec_reward\n",
        "\n",
        "        # Gym requires step must return a scalar reward。\n",
        "        # but we won't use it in our training\n",
        "        scalar_reward_log = r1 + r2\n",
        "\n",
        "        new_obs = np.concatenate([obs, self.current_w])\n",
        "        return new_obs, scalar_reward_log, terminated, truncated, info\n",
        "\n",
        "# ==========================================\n",
        "# 2. Vector-Critic Agent\n",
        "# ==========================================\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "class VectorAgent(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        # Critic\n",
        "        # Input: 6 (State + w)\n",
        "        # Output: 2 (V1 and V2)\n",
        "        self.critic = nn.Sequential(\n",
        "            layer_init(nn.Linear(6, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 2), std=1.0), # 2D\n",
        "        )\n",
        "        # Actor: the same\n",
        "        self.actor = nn.Sequential(\n",
        "            layer_init(nn.Linear(6, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, env.action_space.n), std=0.01),\n",
        "        )\n",
        "\n",
        "    def get_value(self, x):\n",
        "        # shape: [batch_size, 2]\n",
        "        return self.critic(x)\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        logits = self.actor(x)\n",
        "        probs = Categorical(logits=logits)\n",
        "        if action is None:\n",
        "            action = probs.sample()\n",
        "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
        "\n",
        "# ==========================================\n",
        "# 3. main loop\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    # hyperparameters\n",
        "    learning_rate = 3e-4\n",
        "    num_steps = 128\n",
        "    total_timesteps = 80000\n",
        "    gamma = 0.99\n",
        "    gae_lambda = 0.95\n",
        "    update_epochs = 4\n",
        "    clip_coef = 0.2\n",
        "    ent_coef = 0.001\n",
        "    vf_coef = 0.5\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # initialize env and VectorAgent\n",
        "    env = SteerableCartPoleWrapper(gym.make(\"CartPole-v1\"))\n",
        "    agent = VectorAgent(env).to(device)\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
        "\n",
        "    # initialize Buffer\n",
        "    obs = torch.zeros((num_steps, 6)).to(device)\n",
        "    actions = torch.zeros((num_steps,)).to(device)\n",
        "    logprobs = torch.zeros((num_steps,)).to(device)\n",
        "    # Rewards and Values becomes [num_steps, 2] (2D)\n",
        "    rewards = torch.zeros((num_steps, 2)).to(device)\n",
        "    values = torch.zeros((num_steps, 2)).to(device)\n",
        "    dones = torch.zeros((num_steps,)).to(device)\n",
        "\n",
        "    # save Context w for each step\n",
        "    contexts = torch.zeros((num_steps, 2)).to(device)\n",
        "\n",
        "    # initailize\n",
        "    global_step = 0\n",
        "    next_obs, _ = env.reset()\n",
        "    next_obs = torch.Tensor(next_obs).to(device)\n",
        "    next_done = torch.tensor(0.0).to(device)\n",
        "    num_updates = total_timesteps // num_steps\n",
        "\n",
        "    print(\"Starting Training (Variant B: Q-Space Scalarization)...\")\n",
        "\n",
        "    for update in range(1, num_updates + 1):\n",
        "\n",
        "        # 1.  Rollout\n",
        "        for step in range(num_steps):\n",
        "            global_step += 1\n",
        "            obs[step] = next_obs\n",
        "            dones[step] = next_done\n",
        "            # save w (State - last two pos)\n",
        "            contexts[step] = next_obs[-2:]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # Value is a vector [2]\n",
        "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
        "                values[step] = value.flatten()\n",
        "\n",
        "            actions[step] = action\n",
        "            logprobs[step] = logprob\n",
        "\n",
        "            real_next_obs, _, terminated, truncated, info = env.step(action.item())\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # get Vector Reward from info\n",
        "            r_vec = info['vec_reward']\n",
        "            rewards[step] = torch.tensor(r_vec).to(device)\n",
        "\n",
        "            next_obs = torch.Tensor(real_next_obs).to(device)\n",
        "            next_done = torch.tensor(float(done)).to(device)\n",
        "\n",
        "            if done:\n",
        "                next_obs_np, _ = env.reset()\n",
        "                next_obs = torch.Tensor(next_obs_np).to(device)\n",
        "\n",
        "        # 2. Vector GAE\n",
        "        with torch.no_grad():\n",
        "            # get next state Vector Value\n",
        "            next_value = agent.get_value(next_obs).reshape(1, -1) # [1, 2]\n",
        "            advantages = torch.zeros_like(rewards).to(device) # [128, 2]\n",
        "            lastgaelam = torch.zeros(2).to(device) # [2]\n",
        "\n",
        "            # calculate Advantage\n",
        "            for t in reversed(range(num_steps)):\n",
        "                if t == num_steps - 1:\n",
        "                    nextnonterminal = 1.0 - next_done\n",
        "                    nextvalues = next_value[0]\n",
        "                else:\n",
        "                    nextnonterminal = 1.0 - dones[t + 1]\n",
        "                    nextvalues = values[t + 1]\n",
        "\n",
        "                # TD Error\n",
        "                # delta -- 2D\n",
        "                delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
        "\n",
        "                # GAE recursive\n",
        "                advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
        "\n",
        "            # Returns = Advantage + Value\n",
        "            returns = advantages + values\n",
        "\n",
        "        # 3. Scalarization & PPO Update\n",
        "\n",
        "        # advantages shape: [128, 2] (advantage for each objective)\n",
        "        # contexts shape:   [128, 2] (w)\n",
        "\n",
        "        # Scalarization:\n",
        "        # Scalar Advantage = w1 * A1 + w2 * A2\n",
        "        scalar_advantages = (advantages * contexts).sum(dim=1)\n",
        "\n",
        "        # Batch data prep\n",
        "        b_obs = obs\n",
        "        b_logprobs = logprobs\n",
        "        b_actions = actions\n",
        "        b_returns = returns             # Value Loss - Vector Returns\n",
        "        b_values = values               # Value - Vector\n",
        "        b_scalar_advantages = scalar_advantages # Policy Loss - Scalar Advantage\n",
        "\n",
        "        for epoch in range(update_epochs):\n",
        "            _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs, b_actions)\n",
        "            logratio = newlogprob - b_logprobs\n",
        "            ratio = logratio.exp()\n",
        "\n",
        "            # standardize Advantage\n",
        "            mb_advantages = (b_scalar_advantages - b_scalar_advantages.mean()) / (b_scalar_advantages.std() + 1e-8)\n",
        "\n",
        "            #  Policy Loss (Scalar Advantage)\n",
        "            pg_loss1 = -mb_advantages * ratio\n",
        "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
        "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "            #  Value Loss ( Vector Returns) ---\n",
        "            # newvalue: [128, 2], b_returns: [128, 2]\n",
        "            # MSE Loss\n",
        "            v_loss = 0.5 * ((newvalue - b_returns) ** 2).mean()\n",
        "\n",
        "            # total Loss\n",
        "            loss = pg_loss - ent_coef * entropy.mean() + vf_coef * v_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if update % 20 == 0:\n",
        "            # print actual Scalar Reward instead of Advantage\n",
        "            train_scalar_rewards = (rewards * contexts).sum(dim=1).mean().item()\n",
        "            print(f\"Update {update}/{num_updates}, Loss: {loss.item():.4f}, Mean Scalar Reward: {train_scalar_rewards:.2f}\")\n",
        "\n",
        "    print(\"Training Finished!\")\n",
        "    torch.save(agent.state_dict(), \"variant_b_agent.pth\")\n",
        "\n",
        "    # ==========================================\n",
        "    # 4. Verification\n",
        "    # ==========================================\n",
        "    print(\"\\n=== Running Verification ===\")\n",
        "    try:\n",
        "        demo_env = SteerableCartPoleWrapper(gym.make(\"CartPole-v1\", render_mode=\"human\"))\n",
        "    except:\n",
        "        demo_env = SteerableCartPoleWrapper(gym.make(\"CartPole-v1\"))\n",
        "\n",
        "    agent.eval()\n",
        "\n",
        "    test_weights = [\n",
        "        [1.0, 0.0],\n",
        "        [0.7, 0.3],\n",
        "        [0.5, 0.5],\n",
        "        [0.3, 0.7],\n",
        "        [0.0, 1.0]\n",
        "    ]\n",
        "\n",
        "    print(f\"{'Weight (w1, w2)':<20} | {'Avg Position':<15} | {'Steps':<10}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for w in test_weights:\n",
        "        obs, _ = demo_env.reset(options={'w': w})\n",
        "        obs = torch.Tensor(obs).to(device)\n",
        "\n",
        "        positions = []\n",
        "        steps = 0\n",
        "\n",
        "        while True:\n",
        "            with torch.no_grad():\n",
        "                action, _, _, _ = agent.get_action_and_value(obs)\n",
        "\n",
        "            step_result = demo_env.step(action.item())\n",
        "            real_next_obs = step_result[0]\n",
        "            term = step_result[2]\n",
        "            trunc = step_result[3]\n",
        "\n",
        "            positions.append(real_next_obs[0])\n",
        "            steps += 1\n",
        "\n",
        "            obs = torch.Tensor(real_next_obs).to(device)\n",
        "            if term or trunc: break\n",
        "\n",
        "        avg_pos = np.mean(positions)\n",
        "        print(f\"{str(w):<20} | {avg_pos: .4f}          | {steps:<10}\")\n",
        "\n",
        "    demo_env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPtzZE8aKF8P",
        "outputId": "b9bd6368-52de-4f83-fbe5-847e4a5a805e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Starting Training (Variant B: Q-Space Scalarization)...\n",
            "Update 20/625, Loss: 0.8657, Mean Scalar Reward: 0.23\n",
            "Update 40/625, Loss: 1.1028, Mean Scalar Reward: 0.25\n",
            "Update 60/625, Loss: 1.6978, Mean Scalar Reward: 0.23\n",
            "Update 80/625, Loss: 3.4439, Mean Scalar Reward: 0.24\n",
            "Update 100/625, Loss: 3.4219, Mean Scalar Reward: 0.27\n",
            "Update 120/625, Loss: 1.6548, Mean Scalar Reward: 0.32\n",
            "Update 140/625, Loss: 2.1339, Mean Scalar Reward: 0.36\n",
            "Update 160/625, Loss: 1.3735, Mean Scalar Reward: 0.25\n",
            "Update 180/625, Loss: 3.6503, Mean Scalar Reward: 0.26\n",
            "Update 200/625, Loss: 1.9251, Mean Scalar Reward: 0.33\n",
            "Update 220/625, Loss: 2.3691, Mean Scalar Reward: 0.25\n",
            "Update 240/625, Loss: 0.8280, Mean Scalar Reward: 0.21\n",
            "Update 260/625, Loss: 2.3836, Mean Scalar Reward: 0.31\n",
            "Update 280/625, Loss: 4.5013, Mean Scalar Reward: 0.27\n",
            "Update 300/625, Loss: 0.2873, Mean Scalar Reward: 0.25\n",
            "Update 320/625, Loss: 0.2973, Mean Scalar Reward: 0.25\n",
            "Update 340/625, Loss: 0.2667, Mean Scalar Reward: 0.25\n",
            "Update 360/625, Loss: 0.3010, Mean Scalar Reward: 0.26\n",
            "Update 380/625, Loss: 4.2395, Mean Scalar Reward: 0.43\n",
            "Update 400/625, Loss: 6.3949, Mean Scalar Reward: 0.25\n",
            "Update 420/625, Loss: 0.2550, Mean Scalar Reward: 0.25\n",
            "Update 440/625, Loss: 4.4311, Mean Scalar Reward: 0.30\n",
            "Update 460/625, Loss: 0.2011, Mean Scalar Reward: 0.26\n",
            "Update 480/625, Loss: 0.1855, Mean Scalar Reward: 0.25\n",
            "Update 500/625, Loss: 0.5218, Mean Scalar Reward: 0.27\n",
            "Update 520/625, Loss: 0.2140, Mean Scalar Reward: 0.27\n",
            "Update 540/625, Loss: 0.4199, Mean Scalar Reward: 0.28\n",
            "Update 560/625, Loss: 6.4443, Mean Scalar Reward: 0.37\n",
            "Update 580/625, Loss: 6.2853, Mean Scalar Reward: 0.27\n",
            "Update 600/625, Loss: 0.2551, Mean Scalar Reward: 0.26\n",
            "Update 620/625, Loss: 0.1125, Mean Scalar Reward: 0.25\n",
            "Training Finished!\n",
            "\n",
            "=== Running Verification ===\n",
            "Weight (w1, w2)      | Avg Position    | Steps     \n",
            "--------------------------------------------------\n",
            "[1.0, 0.0]           | -0.6659          | 500       \n",
            "[0.7, 0.3]           | -0.2094          | 225       \n",
            "[0.5, 0.5]           | -0.0969          | 500       \n",
            "[0.3, 0.7]           |  0.1339          | 99        \n",
            "[0.0, 1.0]           |  0.5791          | 418       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. Variant C: Variant C: Gradient-Space Combination\n",
        "# ==========================================\n",
        "# reuse SteerableCartPoleWrapper and VectorAgent class\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    learning_rate = 3e-4\n",
        "    num_steps = 128\n",
        "    total_timesteps = 80000\n",
        "    gamma = 0.99\n",
        "    gae_lambda = 0.95\n",
        "    update_epochs = 4\n",
        "    clip_coef = 0.2\n",
        "    ent_coef = 0.001\n",
        "    vf_coef = 0.5\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    env = SteerableCartPoleWrapper(gym.make(\"CartPole-v1\"))\n",
        "    agent = VectorAgent(env).to(device)\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Initialize Buffer\n",
        "    obs = torch.zeros((num_steps, 6)).to(device)\n",
        "    actions = torch.zeros((num_steps,)).to(device)\n",
        "    logprobs = torch.zeros((num_steps,)).to(device)\n",
        "    rewards = torch.zeros((num_steps, 2)).to(device)\n",
        "    values = torch.zeros((num_steps, 2)).to(device)\n",
        "    dones = torch.zeros((num_steps,)).to(device)\n",
        "    contexts = torch.zeros((num_steps, 2)).to(device)\n",
        "\n",
        "    global_step = 0\n",
        "    next_obs, _ = env.reset()\n",
        "    next_obs = torch.Tensor(next_obs).to(device)\n",
        "    next_done = torch.tensor(0.0).to(device)\n",
        "    num_updates = total_timesteps // num_steps\n",
        "\n",
        "    print(\"Starting Training (Variant C: Gradient Mixing)...\")\n",
        "\n",
        "    for update in range(1, num_updates + 1):\n",
        "\n",
        "        for step in range(num_steps):\n",
        "            global_step += 1\n",
        "            obs[step] = next_obs\n",
        "            dones[step] = next_done\n",
        "            contexts[step] = next_obs[-2:]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
        "                values[step] = value.flatten()\n",
        "\n",
        "            actions[step] = action\n",
        "            logprobs[step] = logprob\n",
        "\n",
        "            real_next_obs, _, terminated, truncated, info = env.step(action.item())\n",
        "            done = terminated or truncated\n",
        "\n",
        "            r_vec = info['vec_reward']\n",
        "            rewards[step] = torch.tensor(r_vec).to(device)\n",
        "\n",
        "            next_obs = torch.Tensor(real_next_obs).to(device)\n",
        "            next_done = torch.tensor(float(done)).to(device)\n",
        "\n",
        "            if done:\n",
        "                next_obs_np, _ = env.reset()\n",
        "                next_obs = torch.Tensor(next_obs_np).to(device)\n",
        "\n",
        "        #  Vector GAE (the same as B)\n",
        "        with torch.no_grad():\n",
        "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
        "            advantages = torch.zeros_like(rewards).to(device)\n",
        "            lastgaelam = torch.zeros(2).to(device)\n",
        "\n",
        "            for t in reversed(range(num_steps)):\n",
        "                if t == num_steps - 1:\n",
        "                    nextnonterminal = 1.0 - next_done\n",
        "                    nextvalues = next_value[0]\n",
        "                else:\n",
        "                    nextnonterminal = 1.0 - dones[t + 1]\n",
        "                    nextvalues = values[t + 1]\n",
        "\n",
        "                delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
        "                advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
        "\n",
        "            returns = advantages + values\n",
        "\n",
        "        # Variant C core Update\n",
        "\n",
        "        # we keep Vector Advantage\n",
        "        b_obs = obs\n",
        "        b_logprobs = logprobs\n",
        "        b_actions = actions\n",
        "        b_returns = returns\n",
        "        b_values = values\n",
        "        b_vector_advantages = advantages  # [128, 2]\n",
        "        b_contexts = contexts             # [128, 2]\n",
        "\n",
        "        for epoch in range(update_epochs):\n",
        "            _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs, b_actions)\n",
        "            logratio = newlogprob - b_logprobs\n",
        "            ratio = logratio.exp()\n",
        "\n",
        "            # calculaye loss respectively and then add weight\n",
        "            total_pg_loss = 0\n",
        "\n",
        "            for k in range(2):\n",
        "                # get kth Advantage\n",
        "                adv_k = b_vector_advantages[:, k]\n",
        "\n",
        "                # Normalization\n",
        "                adv_k = (adv_k - adv_k.mean()) / (adv_k.std() + 1e-8)\n",
        "\n",
        "                # get the Clip Loss for kth objective\n",
        "                pg_loss1 = -adv_k * ratio\n",
        "                pg_loss2 = -adv_k * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
        "                loss_k = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "                # get weight\n",
        "                w_k = b_contexts[:, k].mean()\n",
        "\n",
        "                # Loss\n",
        "                total_pg_loss += w_k * loss_k\n",
        "\n",
        "            # Value Loss - Vector MSE\n",
        "            v_loss = 0.5 * ((newvalue - b_returns) ** 2).mean()\n",
        "\n",
        "            # total Loss\n",
        "            loss = total_pg_loss - ent_coef * entropy.mean() + vf_coef * v_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if update % 20 == 0:\n",
        "            train_scalar_rewards = (rewards * contexts).sum(dim=1).mean().item()\n",
        "            print(f\"Update {update}/{num_updates}, Loss: {loss.item():.4f}, Mean Scalar Reward: {train_scalar_rewards:.2f}\")\n",
        "\n",
        "    print(\"Training Finished (Variant C)!\")\n",
        "    torch.save(agent.state_dict(), \"variant_c_agent.pth\")\n",
        "\n",
        "    # ==========================================\n",
        "    # Verification\n",
        "    # ==========================================\n",
        "    print(\"\\n=== Running Verification (Variant C) ===\")\n",
        "    try:\n",
        "        demo_env = SteerableCartPoleWrapper(gym.make(\"CartPole-v1\"))\n",
        "    except:\n",
        "        demo_env = SteerableCartPoleWrapper(gym.make(\"CartPole-v1\"))\n",
        "\n",
        "    agent.eval()\n",
        "\n",
        "    test_weights = [\n",
        "        [1.0, 0.0],\n",
        "        [0.7, 0.3],\n",
        "        [0.5, 0.5],\n",
        "        [0.3, 0.7],\n",
        "        [0.0, 1.0]\n",
        "    ]\n",
        "\n",
        "    print(f\"{'Weight (w1, w2)':<20} | {'Avg Position':<15} | {'Steps':<10}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for w in test_weights:\n",
        "        obs, _ = demo_env.reset(options={'w': w})\n",
        "        obs = torch.Tensor(obs).to(device)\n",
        "\n",
        "        positions = []\n",
        "        steps = 0\n",
        "\n",
        "        while True:\n",
        "            with torch.no_grad():\n",
        "                action, _, _, _ = agent.get_action_and_value(obs)\n",
        "\n",
        "            step_result = demo_env.step(action.item())\n",
        "            real_next_obs = step_result[0]\n",
        "            term = step_result[2]\n",
        "            trunc = step_result[3]\n",
        "\n",
        "            positions.append(real_next_obs[0])\n",
        "            steps += 1\n",
        "\n",
        "            obs = torch.Tensor(real_next_obs).to(device)\n",
        "            if term or trunc: break\n",
        "\n",
        "        avg_pos = np.mean(positions)\n",
        "        print(f\"{str(w):<20} | {avg_pos: .4f}          | {steps:<10}\")\n",
        "\n",
        "    demo_env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMI0vmAfWYyh",
        "outputId": "377bc782-c16d-408c-9fbf-cb22658a4afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Starting Training (Variant C: Gradient Mixing)...\n",
            "Update 20/625, Loss: 0.5886, Mean Scalar Reward: 0.24\n",
            "Update 40/625, Loss: 1.7689, Mean Scalar Reward: 0.26\n",
            "Update 60/625, Loss: 1.6883, Mean Scalar Reward: 0.27\n",
            "Update 80/625, Loss: 2.5371, Mean Scalar Reward: 0.23\n",
            "Update 100/625, Loss: 3.6777, Mean Scalar Reward: 0.27\n",
            "Update 120/625, Loss: 2.4311, Mean Scalar Reward: 0.29\n",
            "Update 140/625, Loss: 4.6784, Mean Scalar Reward: 0.23\n",
            "Update 160/625, Loss: 1.4494, Mean Scalar Reward: 0.29\n",
            "Update 180/625, Loss: 0.5523, Mean Scalar Reward: 0.28\n",
            "Update 200/625, Loss: 4.7431, Mean Scalar Reward: 0.24\n",
            "Update 220/625, Loss: 6.2157, Mean Scalar Reward: 0.21\n",
            "Update 240/625, Loss: 5.5149, Mean Scalar Reward: 0.29\n",
            "Update 260/625, Loss: 0.8777, Mean Scalar Reward: 0.22\n",
            "Update 280/625, Loss: 1.7413, Mean Scalar Reward: 0.34\n",
            "Update 300/625, Loss: 1.5268, Mean Scalar Reward: 0.37\n",
            "Update 320/625, Loss: 2.3168, Mean Scalar Reward: 0.29\n",
            "Update 340/625, Loss: 0.1880, Mean Scalar Reward: 0.25\n",
            "Update 360/625, Loss: 0.4993, Mean Scalar Reward: 0.22\n",
            "Update 380/625, Loss: 5.0231, Mean Scalar Reward: 0.25\n",
            "Update 400/625, Loss: 0.1538, Mean Scalar Reward: 0.25\n",
            "Update 420/625, Loss: 0.1878, Mean Scalar Reward: 0.26\n",
            "Update 440/625, Loss: 0.1762, Mean Scalar Reward: 0.27\n",
            "Update 460/625, Loss: 1.0962, Mean Scalar Reward: 0.27\n",
            "Update 480/625, Loss: 0.1261, Mean Scalar Reward: 0.25\n",
            "Update 500/625, Loss: 0.1585, Mean Scalar Reward: 0.24\n",
            "Update 520/625, Loss: 0.7614, Mean Scalar Reward: 0.36\n",
            "Update 540/625, Loss: 0.5654, Mean Scalar Reward: 0.25\n",
            "Update 560/625, Loss: 0.7049, Mean Scalar Reward: 0.36\n",
            "Update 580/625, Loss: 6.2226, Mean Scalar Reward: 0.64\n",
            "Update 600/625, Loss: 0.1336, Mean Scalar Reward: 0.26\n",
            "Update 620/625, Loss: 0.0751, Mean Scalar Reward: 0.25\n",
            "Training Finished (Variant C)!\n",
            "\n",
            "=== Running Verification (Variant C) ===\n",
            "Weight (w1, w2)      | Avg Position    | Steps     \n",
            "--------------------------------------------------\n",
            "[1.0, 0.0]           | -0.7776          | 110       \n",
            "[0.7, 0.3]           | -0.4383          | 277       \n",
            "[0.5, 0.5]           | -0.1897          | 500       \n",
            "[0.3, 0.7]           |  0.3385          | 452       \n",
            "[0.0, 1.0]           |  0.7439          | 341       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "class SteerableWalkerWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        original_shape = env.observation_space.shape[0]\n",
        "\n",
        "        # === 修改点 1: 变成 3 个目标 ===\n",
        "        self.num_objectives = 3\n",
        "\n",
        "        # Observation = State(17) + Preference(3) = 20\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=-np.inf, high=np.inf,\n",
        "            shape=(original_shape + self.num_objectives,),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "        # 初始化权重 (w1, w2, w3)\n",
        "        self.current_w = np.array([0.33, 0.33, 0.33], dtype=np.float32)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        if options and 'w' in options:\n",
        "            self.current_w = np.array(options['w'], dtype=np.float32)\n",
        "        else:\n",
        "            # 随机采样 3 个权重并归一化\n",
        "            w = np.random.rand(self.num_objectives)\n",
        "            self.current_w = w / w.sum()\n",
        "\n",
        "        obs, info = self.env.reset(seed=seed)\n",
        "        return np.concatenate([obs, self.current_w]), info\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, _, terminated, truncated, info = self.env.step(action)\n",
        "\n",
        "        # === 修改点 2: 提取 3 个分量 ===\n",
        "        # 1. 速度 (Forward Reward)\n",
        "        r_velocity = info.get('reward_forward', 0.0)\n",
        "\n",
        "        # 2. 存活 (Healthy Reward)\n",
        "        # 注意: 倒下时这个通常就没有了，或者为0\n",
        "        r_survive = info.get('reward_survive', 0.0)\n",
        "\n",
        "        # 3. 能耗 (Control Cost)\n",
        "        # Gym 返回的是正数的 cost，我们要优化的目标是\"负的cost\" (即 maximize -cost)\n",
        "        r_energy = -info.get('reward_ctrl', 0.0)\n",
        "\n",
        "        # 组装成 3维 向量\n",
        "        vec_reward = np.array([r_velocity, r_survive, r_energy], dtype=np.float32)\n",
        "\n",
        "        # 存入 info 供 Variant B/C 使用\n",
        "        info['vec_reward'] = vec_reward\n",
        "\n",
        "        # 计算标量奖励 (供 Variant A 使用，或者作为 log)\n",
        "        # Scalar = w1*Vel + w2*Survive + w3*Energy\n",
        "        weighted_scalar_reward = np.dot(self.current_w, vec_reward)\n",
        "\n",
        "        new_obs = np.concatenate([obs, self.current_w])\n",
        "\n",
        "        return new_obs, weighted_scalar_reward, terminated, truncated, info"
      ],
      "metadata": {
        "id": "2Tr0EolgADkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "# ... (请确保最上面已经包含了 SteerableWalkerWrapper 类) ...\n",
        "\n",
        "# ==========================================\n",
        "# 2. Continuous Scalar Agent (Variant A 专用)\n",
        "# ==========================================\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "class ContinuousScalarAgent(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        # Observation = 17 (state) + 3 (weights) = 20\n",
        "        obs_dim = env.observation_space.shape[0]\n",
        "        action_dim = env.action_space.shape[0] # Walker2d = 6\n",
        "\n",
        "        # === Critic (Scalar Output) ===\n",
        "        # Variant A: Critic 只预测一个标量 Value\n",
        "        self.critic = nn.Sequential(\n",
        "            layer_init(nn.Linear(obs_dim, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 1), std=1.0),\n",
        "        )\n",
        "\n",
        "        # === Actor (Continuous Mean) ===\n",
        "        # 和 Variant B 一模一样\n",
        "        self.actor_mean = nn.Sequential(\n",
        "            layer_init(nn.Linear(obs_dim, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, action_dim), std=0.01),\n",
        "        )\n",
        "        self.actor_logstd = nn.Parameter(torch.zeros(1, action_dim))\n",
        "\n",
        "    def get_value(self, x):\n",
        "        return self.critic(x)\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        action_mean = self.actor_mean(x)\n",
        "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
        "        action_std = torch.exp(action_logstd)\n",
        "\n",
        "        probs = Normal(action_mean, action_std)\n",
        "\n",
        "        if action is None:\n",
        "            action = probs.sample()\n",
        "\n",
        "        log_prob = probs.log_prob(action).sum(1)\n",
        "        entropy = probs.entropy().sum(1)\n",
        "\n",
        "        return action, log_prob, entropy, self.critic(x)\n",
        "\n",
        "# ==========================================\n",
        "# 3. Main Training Loop (Variant A: Reward Scalarization)\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    # === Hyperparameters ===\n",
        "    learning_rate = 3e-4\n",
        "    num_steps = 2048\n",
        "    total_timesteps = 1000000\n",
        "    gamma = 0.99\n",
        "    gae_lambda = 0.95\n",
        "    update_epochs = 10\n",
        "    clip_coef = 0.2\n",
        "    ent_coef = 0.0\n",
        "    vf_coef = 0.5\n",
        "    max_grad_norm = 0.5\n",
        "    batch_size = 64\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize Env\n",
        "    env = SteerableWalkerWrapper(gym.make(\"Walker2d-v4\"))\n",
        "\n",
        "    # 使用 Scalar Agent\n",
        "    agent = ContinuousScalarAgent(env).to(device)\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Initialize Buffers\n",
        "    obs = torch.zeros((num_steps, env.observation_space.shape[0])).to(device)\n",
        "    actions = torch.zeros((num_steps, env.action_space.shape[0])).to(device)\n",
        "    logprobs = torch.zeros((num_steps,)).to(device)\n",
        "\n",
        "    # Variant A: Rewards 和 Values 都是 1维 Scalar\n",
        "    rewards = torch.zeros((num_steps,)).to(device)\n",
        "    values = torch.zeros((num_steps,)).to(device)\n",
        "    dones = torch.zeros((num_steps,)).to(device)\n",
        "\n",
        "    # 依然需要记录 w 只是为了打印 log 或者 debug，但不需要参与计算\n",
        "    # 这里我们不用 contexts 来算 loss，因为 reward 已经是加权好的了\n",
        "\n",
        "    global_step = 0\n",
        "    next_obs, _ = env.reset()\n",
        "    next_obs = torch.Tensor(next_obs).to(device)\n",
        "    next_done = torch.tensor(0.0).to(device)\n",
        "\n",
        "    num_updates = total_timesteps // num_steps\n",
        "\n",
        "    print(f\"Starting Walker2d Training (Variant A)... Target: {total_timesteps} steps\")\n",
        "\n",
        "    for update in range(1, num_updates + 1):\n",
        "\n",
        "        # 1. Rollout\n",
        "        for step in range(num_steps):\n",
        "            global_step += 1\n",
        "            obs[step] = next_obs\n",
        "            dones[step] = next_done\n",
        "\n",
        "            with torch.no_grad():\n",
        "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
        "                values[step] = value.flatten() # 变成标量\n",
        "\n",
        "            actions[step] = action\n",
        "            logprobs[step] = logprob\n",
        "\n",
        "            # Step\n",
        "            real_next_obs, scalar_reward, terminated, truncated, info = env.step(action.cpu().numpy())\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Variant A: 直接存 Wrapper 算好的 scalar_reward\n",
        "            rewards[step] = torch.tensor(scalar_reward).to(device)\n",
        "\n",
        "            next_obs = torch.Tensor(real_next_obs).to(device)\n",
        "            next_done = torch.tensor(float(done)).to(device)\n",
        "\n",
        "            if done:\n",
        "                next_obs_np, _ = env.reset()\n",
        "                next_obs = torch.Tensor(next_obs_np).to(device)\n",
        "\n",
        "        # 2. Standard Scalar GAE\n",
        "        with torch.no_grad():\n",
        "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
        "            advantages = torch.zeros_like(rewards).to(device)\n",
        "            lastgaelam = 0\n",
        "\n",
        "            for t in reversed(range(num_steps)):\n",
        "                if t == num_steps - 1:\n",
        "                    nextnonterminal = 1.0 - next_done\n",
        "                    nextvalues = next_value\n",
        "                else:\n",
        "                    nextnonterminal = 1.0 - dones[t + 1]\n",
        "                    nextvalues = values[t + 1]\n",
        "\n",
        "                # Scalar Delta\n",
        "                delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
        "                advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
        "\n",
        "            returns = advantages + values\n",
        "\n",
        "        # 3. Standard PPO Update\n",
        "        b_obs = obs.reshape((-1, env.observation_space.shape[0]))\n",
        "        b_logprobs = logprobs.reshape(-1)\n",
        "        b_actions = actions.reshape((-1, env.action_space.shape[0]))\n",
        "        b_advantages = advantages.reshape(-1)\n",
        "        b_returns = returns.reshape(-1)\n",
        "        b_values = values.reshape(-1)\n",
        "\n",
        "        b_inds = np.arange(num_steps)\n",
        "\n",
        "        for epoch in range(update_epochs):\n",
        "            np.random.shuffle(b_inds)\n",
        "            for start in range(0, num_steps, batch_size):\n",
        "                end = start + batch_size\n",
        "                mb_inds = b_inds[start:end]\n",
        "\n",
        "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])\n",
        "                logratio = newlogprob - b_logprobs[mb_inds]\n",
        "                ratio = logratio.exp()\n",
        "\n",
        "                mb_adv = b_advantages[mb_inds]\n",
        "                mb_adv = (mb_adv - mb_adv.mean()) / (mb_adv.std() + 1e-8)\n",
        "\n",
        "                pg_loss1 = -mb_adv * ratio\n",
        "                pg_loss2 = -mb_adv * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
        "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "                # Value Loss (Scalar MSE)\n",
        "                v_loss = 0.5 * ((newvalue.view(-1) - b_returns[mb_inds]) ** 2).mean()\n",
        "\n",
        "                loss = pg_loss - ent_coef * entropy.mean() + vf_coef * v_loss\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
        "                optimizer.step()\n",
        "\n",
        "        if update % 10 == 0:\n",
        "            print(f\"Update {update}/{num_updates} | Loss: {loss.item():.4f} | Mean Reward: {rewards.mean().item():.2f}\")\n",
        "\n",
        "    print(\"Training Finished (Variant A)!\")\n",
        "    torch.save(agent.state_dict(), \"walker_variant_a_scalar.pth\")"
      ],
      "metadata": {
        "id": "zTxlNWP_qvVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "# ... (你的 SteerableWalkerWrapper 代码应该在上面) ...\n",
        "\n",
        "# ==========================================\n",
        "# 2. Continuous Vector-Critic Agent (Walker2d专用)\n",
        "# ==========================================\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "class ContinuousVectorAgent(nn.Module):\n",
        "    def __init__(self, env, num_objectives=3):\n",
        "        super().__init__()\n",
        "        # 获取维度\n",
        "        # Observation = 17 (state) + 3 (weights) = 20\n",
        "        obs_dim = env.observation_space.shape[0]\n",
        "        action_dim = env.action_space.shape[0] # Walker2d = 6\n",
        "\n",
        "        self.num_objectives = num_objectives\n",
        "\n",
        "        # === Critic (Vector Output) ===\n",
        "        # 输出维度 = 3 (Vel, Survive, Energy)\n",
        "        self.critic = nn.Sequential(\n",
        "            layer_init(nn.Linear(obs_dim, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, num_objectives), std=1.0),\n",
        "        )\n",
        "\n",
        "        # === Actor (Continuous Mean) ===\n",
        "        self.actor_mean = nn.Sequential(\n",
        "            layer_init(nn.Linear(obs_dim, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, 64)),\n",
        "            nn.Tanh(),\n",
        "            layer_init(nn.Linear(64, action_dim), std=0.01),\n",
        "        )\n",
        "        # Learnable Log Std (不依赖状态，是一个独立参数)\n",
        "        self.actor_logstd = nn.Parameter(torch.zeros(1, action_dim))\n",
        "\n",
        "    def get_value(self, x):\n",
        "        # returns [batch, 3]\n",
        "        return self.critic(x)\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        action_mean = self.actor_mean(x)\n",
        "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
        "        action_std = torch.exp(action_logstd)\n",
        "\n",
        "        # 使用正态分布\n",
        "        probs = Normal(action_mean, action_std)\n",
        "\n",
        "        if action is None:\n",
        "            action = probs.sample()\n",
        "\n",
        "        # 连续动作空间的 log_prob 需要对所有维度求和\n",
        "        log_prob = probs.log_prob(action).sum(1)\n",
        "        entropy = probs.entropy().sum(1)\n",
        "\n",
        "        return action, log_prob, entropy, self.critic(x)\n",
        "\n",
        "# ==========================================\n",
        "# 3. Main Training Loop (Variant B)\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    # === Hyperparameters for MuJoCo ===\n",
        "    learning_rate = 3e-4\n",
        "    num_steps = 2048           # 增加: MuJoCo 需要更长的 horizon\n",
        "    total_timesteps = 1000000  # 增加: 1M steps\n",
        "    gamma = 0.99\n",
        "    gae_lambda = 0.95\n",
        "    update_epochs = 10         # 增加: 稍微多一点 epoch\n",
        "    clip_coef = 0.2\n",
        "    ent_coef = 0.0             # 连续控制通常不需要太高的熵系数\n",
        "    vf_coef = 0.5\n",
        "    max_grad_norm = 0.5\n",
        "    batch_size = 64            # Minibatch size\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize Env\n",
        "    # 确保你的 Wrapper 名字是对的\n",
        "    env = SteerableWalkerWrapper(gym.make(\"Walker2d-v4\"))\n",
        "\n",
        "    agent = ContinuousVectorAgent(env, num_objectives=3).to(device)\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Initialize Buffers\n",
        "    # Obs 维度包含权重\n",
        "    obs = torch.zeros((num_steps, env.observation_space.shape[0])).to(device)\n",
        "    # Action 维度不再是 scalar，而是 vector [num_steps, 6]\n",
        "    actions = torch.zeros((num_steps, env.action_space.shape[0])).to(device)\n",
        "    logprobs = torch.zeros((num_steps,)).to(device)\n",
        "\n",
        "    # Rewards/Values 变成 [num_steps, 3]\n",
        "    rewards = torch.zeros((num_steps, 3)).to(device)\n",
        "    values = torch.zeros((num_steps, 3)).to(device)\n",
        "\n",
        "    dones = torch.zeros((num_steps,)).to(device)\n",
        "    contexts = torch.zeros((num_steps, 3)).to(device) # save weights\n",
        "\n",
        "    global_step = 0\n",
        "    next_obs, _ = env.reset()\n",
        "    next_obs = torch.Tensor(next_obs).to(device)\n",
        "    next_done = torch.tensor(0.0).to(device)\n",
        "\n",
        "    num_updates = total_timesteps // num_steps\n",
        "\n",
        "    print(f\"Starting Walker2d Training (3 Objectives)... Target: {total_timesteps} steps\")\n",
        "\n",
        "    for update in range(1, num_updates + 1):\n",
        "\n",
        "        # 1. Rollout Phase\n",
        "        for step in range(num_steps):\n",
        "            global_step += 1\n",
        "            obs[step] = next_obs\n",
        "            dones[step] = next_done\n",
        "            # 保存当前的权重 w (Observation 的最后3位)\n",
        "            contexts[step] = next_obs[-3:]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
        "                values[step] = value # [3]\n",
        "\n",
        "            actions[step] = action\n",
        "            logprobs[step] = logprob\n",
        "\n",
        "            # Step environment\n",
        "            # Action 需要转回 numpy\n",
        "            real_next_obs, _, terminated, truncated, info = env.step(action.cpu().numpy())\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # 获取 3D Reward\n",
        "            r_vec = info['vec_reward']\n",
        "            rewards[step] = torch.tensor(r_vec).to(device)\n",
        "\n",
        "            next_obs = torch.Tensor(real_next_obs).to(device)\n",
        "            next_done = torch.tensor(float(done)).to(device)\n",
        "\n",
        "            if done:\n",
        "                next_obs_np, _ = env.reset()\n",
        "                next_obs = torch.Tensor(next_obs_np).to(device)\n",
        "\n",
        "        # 2. Vector GAE Calculation\n",
        "        with torch.no_grad():\n",
        "            next_value = agent.get_value(next_obs) # [3] vector\n",
        "            advantages = torch.zeros_like(rewards).to(device)\n",
        "            lastgaelam = torch.zeros(3).to(device) # [3] vector\n",
        "\n",
        "            for t in reversed(range(num_steps)):\n",
        "                if t == num_steps - 1:\n",
        "                    nextnonterminal = 1.0 - next_done\n",
        "                    nextvalues = next_value\n",
        "                else:\n",
        "                    nextnonterminal = 1.0 - dones[t + 1]\n",
        "                    nextvalues = values[t + 1]\n",
        "\n",
        "                # Vector Delta\n",
        "                delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
        "                advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
        "\n",
        "            returns = advantages + values\n",
        "\n",
        "        # 3. Scalarization & PPO Update\n",
        "        # Variant B: 在 Advantage 层面进行加权求和\n",
        "        # scalar_advantages shape: [num_steps]\n",
        "        scalar_advantages = (advantages * contexts).sum(dim=1)\n",
        "\n",
        "        # Flatten batch\n",
        "        b_obs = obs.reshape((-1, env.observation_space.shape[0]))\n",
        "        b_logprobs = logprobs.reshape(-1)\n",
        "        b_actions = actions.reshape((-1, env.action_space.shape[0]))\n",
        "        b_scalar_advantages = scalar_advantages.reshape(-1)\n",
        "        b_returns = returns.reshape((-1, 3))\n",
        "        b_values = values.reshape((-1, 3))\n",
        "\n",
        "        # Mini-batch Update\n",
        "        # 创建索引\n",
        "        b_inds = np.arange(num_steps)\n",
        "        clipfracs = []\n",
        "\n",
        "        for epoch in range(update_epochs):\n",
        "            np.random.shuffle(b_inds)\n",
        "            for start in range(0, num_steps, batch_size):\n",
        "                end = start + batch_size\n",
        "                mb_inds = b_inds[start:end]\n",
        "\n",
        "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])\n",
        "                logratio = newlogprob - b_logprobs[mb_inds]\n",
        "                ratio = logratio.exp()\n",
        "\n",
        "                mb_adv = b_scalar_advantages[mb_inds]\n",
        "                # Normalize Advantage\n",
        "                mb_adv = (mb_adv - mb_adv.mean()) / (mb_adv.std() + 1e-8)\n",
        "\n",
        "                # Policy Loss\n",
        "                pg_loss1 = -mb_adv * ratio\n",
        "                pg_loss2 = -mb_adv * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
        "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "                # Value Loss (Vector MSE)\n",
        "                # 计算 3个 objective 的平均 MSE\n",
        "                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
        "\n",
        "                loss = pg_loss - ent_coef * entropy.mean() + vf_coef * v_loss\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                # Gradient Clipping\n",
        "                nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
        "                optimizer.step()\n",
        "\n",
        "        if update % 10 == 0:\n",
        "            # 打印当前所有目标加权后的平均奖励\n",
        "            train_scalar_rewards = (rewards * contexts).sum(dim=1).sum().item() / num_steps\n",
        "            print(f\"Update {update}/{num_updates} | Loss: {loss.item():.4f} | Avg Weighted Reward: {train_scalar_rewards:.2f}\")\n",
        "\n",
        "    print(\"Training Finished!\")\n",
        "    torch.save(agent.state_dict(), \"walker_variant_b_3obj.pth\")"
      ],
      "metadata": {
        "id": "6aPW-PlZpyCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "# ... (请确保 SteerableWalkerWrapper 和 ContinuousVectorAgent 类已经在上面定义了) ...\n",
        "# ... (可以直接复用 Variant B 里面定义的 ContinuousVectorAgent) ...\n",
        "\n",
        "# ==========================================\n",
        "# 3. Main Training Loop (Variant C: Gradient Mixing)\n",
        "# ==========================================\n",
        "if __name__ == \"__main__\":\n",
        "    # === Hyperparameters ===\n",
        "    learning_rate = 3e-4\n",
        "    num_steps = 2048\n",
        "    total_timesteps = 1000000\n",
        "    gamma = 0.99\n",
        "    gae_lambda = 0.95\n",
        "    update_epochs = 10\n",
        "    clip_coef = 0.2\n",
        "    ent_coef = 0.0\n",
        "    vf_coef = 0.5\n",
        "    max_grad_norm = 0.5\n",
        "    batch_size = 64\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize Env\n",
        "    env = SteerableWalkerWrapper(gym.make(\"Walker2d-v4\"))\n",
        "\n",
        "    # Variant C 也需要 Vector Critic 来分别估计每个目标的 Advantage\n",
        "    agent = ContinuousVectorAgent(env, num_objectives=3).to(device)\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Initialize Buffers\n",
        "    obs = torch.zeros((num_steps, env.observation_space.shape[0])).to(device)\n",
        "    actions = torch.zeros((num_steps, env.action_space.shape[0])).to(device)\n",
        "    logprobs = torch.zeros((num_steps,)).to(device)\n",
        "\n",
        "    # 3D Rewards & Values\n",
        "    rewards = torch.zeros((num_steps, 3)).to(device)\n",
        "    values = torch.zeros((num_steps, 3)).to(device)\n",
        "    dones = torch.zeros((num_steps,)).to(device)\n",
        "\n",
        "    # 必须保存 context (权重)，用于在 Loss 阶段进行加权\n",
        "    contexts = torch.zeros((num_steps, 3)).to(device)\n",
        "\n",
        "    global_step = 0\n",
        "    next_obs, _ = env.reset()\n",
        "    next_obs = torch.Tensor(next_obs).to(device)\n",
        "    next_done = torch.tensor(0.0).to(device)\n",
        "\n",
        "    num_updates = total_timesteps // num_steps\n",
        "\n",
        "    print(f\"Starting Walker2d Training (Variant C: Multi-Objective Gradient)...\")\n",
        "\n",
        "    for update in range(1, num_updates + 1):\n",
        "\n",
        "        # 1. Rollout (和 Variant B 一样)\n",
        "        for step in range(num_steps):\n",
        "            global_step += 1\n",
        "            obs[step] = next_obs\n",
        "            dones[step] = next_done\n",
        "            contexts[step] = next_obs[-3:]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
        "                values[step] = value\n",
        "\n",
        "            actions[step] = action\n",
        "            logprobs[step] = logprob\n",
        "\n",
        "            real_next_obs, _, terminated, truncated, info = env.step(action.cpu().numpy())\n",
        "            done = terminated or truncated\n",
        "\n",
        "            r_vec = info['vec_reward']\n",
        "            rewards[step] = torch.tensor(r_vec).to(device)\n",
        "\n",
        "            next_obs = torch.Tensor(real_next_obs).to(device)\n",
        "            next_done = torch.tensor(float(done)).to(device)\n",
        "\n",
        "            if done:\n",
        "                next_obs_np, _ = env.reset()\n",
        "                next_obs = torch.Tensor(next_obs_np).to(device)\n",
        "\n",
        "        # 2. Vector GAE (和 Variant B 一样，分别计算 3 个 Advantage)\n",
        "        with torch.no_grad():\n",
        "            next_value = agent.get_value(next_obs)\n",
        "            advantages = torch.zeros_like(rewards).to(device)\n",
        "            lastgaelam = torch.zeros(3).to(device)\n",
        "\n",
        "            for t in reversed(range(num_steps)):\n",
        "                if t == num_steps - 1:\n",
        "                    nextnonterminal = 1.0 - next_done\n",
        "                    nextvalues = next_value\n",
        "                else:\n",
        "                    nextnonterminal = 1.0 - dones[t + 1]\n",
        "                    nextvalues = values[t + 1]\n",
        "\n",
        "                delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
        "                advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
        "\n",
        "            returns = advantages + values\n",
        "\n",
        "        # 3. Variant C Special Update: Gradient Mixing\n",
        "        # 我们保留 3D Advantage，不要在这里做加权求和\n",
        "\n",
        "        b_obs = obs.reshape((-1, env.observation_space.shape[0]))\n",
        "        b_logprobs = logprobs.reshape(-1)\n",
        "        b_actions = actions.reshape((-1, env.action_space.shape[0]))\n",
        "        b_vector_advantages = advantages.reshape((-1, 3)) # [Batch, 3]\n",
        "        b_returns = returns.reshape((-1, 3))\n",
        "        b_contexts = contexts.reshape((-1, 3))            # [Batch, 3]\n",
        "\n",
        "        b_inds = np.arange(num_steps)\n",
        "\n",
        "        for epoch in range(update_epochs):\n",
        "            np.random.shuffle(b_inds)\n",
        "            for start in range(0, num_steps, batch_size):\n",
        "                end = start + batch_size\n",
        "                mb_inds = b_inds[start:end]\n",
        "\n",
        "                # 获取新的 logprob\n",
        "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])\n",
        "                logratio = newlogprob - b_logprobs[mb_inds]\n",
        "                ratio = logratio.exp()\n",
        "\n",
        "                # === Variant C 核心部分 ===\n",
        "                total_pg_loss = 0\n",
        "\n",
        "                # 遍历 3 个目标，分别计算 Loss\n",
        "                for k in range(3):\n",
        "                    # 获取第 k 个目标的 Advantage\n",
        "                    adv_k = b_vector_advantages[mb_inds, k]\n",
        "\n",
        "                    # Per-objective Normalization (对于多目标训练非常重要)\n",
        "                    adv_k = (adv_k - adv_k.mean()) / (adv_k.std() + 1e-8)\n",
        "\n",
        "                    # 计算第 k 个目标的 PPO Loss (Independent Clipping)\n",
        "                    # 注意：这里的 clip 仅受当前目标的 advantage 影响\n",
        "                    loss1 = -adv_k * ratio\n",
        "                    loss2 = -adv_k * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
        "\n",
        "                    # 得到 element-wise 的 loss [batch_size]\n",
        "                    loss_k_element = torch.max(loss1, loss2)\n",
        "\n",
        "                    # 获取第 k 个目标的权重 w_k\n",
        "                    w_k = b_contexts[mb_inds, k]\n",
        "\n",
        "                    # 加权累加 Loss\n",
        "                    # mean() 是对 batch 取平均\n",
        "                    total_pg_loss += (w_k * loss_k_element).mean()\n",
        "\n",
        "                # Value Loss (Vector MSE)\n",
        "                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
        "\n",
        "                # Total Loss\n",
        "                loss = total_pg_loss - ent_coef * entropy.mean() + vf_coef * v_loss\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward() # 这里实际上完成了 Gradients 的加权求和\n",
        "                nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
        "                optimizer.step()\n",
        "\n",
        "        if update % 10 == 0:\n",
        "            # 这里的 log 还是打印加权后的奖励，方便观察\n",
        "            train_scalar_rewards = (rewards * contexts).sum(dim=1).sum().item() / num_steps\n",
        "            print(f\"Update {update}/{num_updates} | Loss: {loss.item():.4f} | Avg Weighted Reward: {train_scalar_rewards:.2f}\")\n",
        "\n",
        "    print(\"Training Finished (Variant C)!\")\n",
        "    torch.save(agent.state_dict(), \"walker_variant_c_grad.pth\")"
      ],
      "metadata": {
        "id": "bF5acIb_rglj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}